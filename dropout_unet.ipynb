{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"dropout_unet.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"ZgW_sm8ChG4M","colab_type":"text"},"cell_type":"markdown","source":["This code implements an all dropout UNET model in Keras. The architecture was inspired by U-Net: Convolutional Networks for Biomedical Image Segmentation base paper. The original dataset is from the RSNA Pneumonia challenge and can be found in the folder data/membrane/train. The test data will be in the 'test' folder and the genrated masks will be in the 'results' folder. The data for training contains 546 images with their respective masks, which are far not enough to feed a deep learning neural network. I have augmented the data using ImageDataGenerator. This deep neural network is implemented with Keras functional API, which makes it extremely easy to experiment with different interesting architectures. The output from the network is a 224 * 224 mask that is learned from the training data. Sigmoid activation function makes sure that mask pixels are in [0, 1] range. The model is trained for 200 epochs with 2000 steps per epoch. After 200 epochs, the calculated accuracy is about 0.98. The loss function for the training is basically just a binary crossentropy. This notebook depends on the following libraries: Tensorflow Keras >= 1.0. Also, this code should be compatible with Python versions 2.7-3.5.\n"]},{"metadata":{"id":"GykzUkwRij6u","colab_type":"code","colab":{}},"cell_type":"code","source":["#load libraries\n","from __future__ import print_function\n","\n","from keras.models import *\n","from keras.layers import *\n","from keras.optimizers import *\n","#from model import *\n","#from data import *\n","from keras.preprocessing.image import ImageDataGenerator\n","import numpy as np \n","import os\n","import skimage.io as io\n","import skimage.transform as trans\n","from PIL import Image\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n","from keras import backend as keras\n","try:\n","    from itertools import izip as zip\n","except ImportError: # will be 3.x series\n","    pass"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nk8UUvGCiugO","colab_type":"text"},"cell_type":"markdown","source":["Define the UNET model. This is an all dropout model as suggested in the base paper. The dropout ratio is fixed to 0.1."]},{"metadata":{"id":"rYNn4zRQizlO","colab_type":"code","colab":{}},"cell_type":"code","source":["def unet(pretrained_weights = None,input_size = (224,224,1)): \n","    inputs = Input(input_size)\n","    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n","    conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n","    drop1 = Dropout(0.1)(conv1)\n","    pool1 = MaxPooling2D(pool_size=(2, 2))(drop1)\n","    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n","    conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n","    drop2 = Dropout(0.1)(conv2)\n","    pool2 = MaxPooling2D(pool_size=(2, 2))(drop2)\n","    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n","    conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n","    drop3 = Dropout(0.1)(conv3)\n","    pool3 = MaxPooling2D(pool_size=(2, 2))(drop3)\n","    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n","    conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n","    drop4 = Dropout(0.1)(conv4)\n","    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n","\n","    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n","    conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n","    drop5 = Dropout(0.1)(conv5)\n","\n","    up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n","    merge6 = Add()([conv4,up6])\n","    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n","    conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n","    drop6 = Dropout(0.1)(conv6)\n","    \n","    up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop6))\n","    merge7 = Add()([conv3,up7])\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n","    conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n","    drop7 = Dropout(0.1)(conv7)\n","    \n","    up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop7))\n","    merge8 = Add()([conv2,up8])\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n","    conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n","    drop8 = Dropout(0.1)(conv8)\n","    \n","    up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop8))\n","    merge9 = Add()([conv1,up9])\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n","    conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n","    drop9 = Dropout(0.1)(conv9)\n","    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(drop9)\n","    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n","    model = Model(inputs = inputs, outputs = conv10)\n","\n","    model.compile(optimizer = Adam(lr = 1e-5), loss = 'binary_crossentropy', metrics = ['accuracy'])\n","    \n","    model.summary()\n","\n","    if(pretrained_weights):\n","    \tmodel.load_weights(pretrained_weights)\n","\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"qiv99YV9i6j4","colab_type":"text"},"cell_type":"markdown","source":["Define the data format and the functions to train and test with the data using image generators. Make sure to use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same if you want to visualize the results of generator, set save_to_dir = \"your path\""]},{"metadata":{"id":"Rdxrszdki-_Q","colab_type":"code","colab":{}},"cell_type":"code","source":["Sky = [128,128,128]\n","Building = [128,0,0]\n","Pole = [192,192,128]\n","Road = [128,64,128]\n","Pavement = [60,40,222]\n","Tree = [128,128,0]\n","SignSymbol = [192,128,128]\n","Fence = [64,64,128]\n","Car = [64,0,128]\n","Pedestrian = [64,64,0]\n","Bicyclist = [0,128,192]\n","Unlabelled = [0,0,0]\n","\n","COLOR_DICT = np.array([Sky, Building, Pole, Road, Pavement,\n","                          Tree, SignSymbol, Fence, Car, Pedestrian, Bicyclist, Unlabelled])\n","\n","\n","def adjustData(img,mask,flag_multi_class,num_class):\n","    if(flag_multi_class):\n","        img = img / 255\n","        mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n","        new_mask = np.zeros(mask.shape + (num_class,))\n","        for i in range(num_class):\n","            new_mask[mask == i,i] = 1\n","        new_mask = np.reshape(new_mask,(new_mask.shape[0],new_mask.shape[1]*new_mask.shape[2],new_mask.shape[3])) if flag_multi_class else np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n","        mask = new_mask\n","    elif(np.max(img) > 1):\n","        img = img / 255\n","        mask = mask /255\n","        mask[mask > 0.5] = 1\n","        mask[mask <= 0.5] = 0\n","    return (img,mask)\n","\n","\n","def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"grayscale\",\n","                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n","                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (224,224),seed = 1):\n","\n","    image_datagen = ImageDataGenerator(**aug_dict)\n","    mask_datagen = ImageDataGenerator(**aug_dict)\n","    image_generator = image_datagen.flow_from_directory(\n","        train_path,\n","        classes = [image_folder],\n","        class_mode = None,\n","        color_mode = image_color_mode,\n","        target_size = target_size,\n","        batch_size = batch_size,\n","        save_to_dir = save_to_dir,\n","        save_prefix  = image_save_prefix,\n","        seed = seed)\n","    mask_generator = mask_datagen.flow_from_directory(\n","        train_path,\n","        classes = [mask_folder],\n","        class_mode = None,\n","        color_mode = mask_color_mode,\n","        target_size = target_size,\n","        batch_size = batch_size,\n","        save_to_dir = save_to_dir,\n","        save_prefix  = mask_save_prefix,\n","        seed = seed)\n","    train_generator = zip(image_generator, mask_generator)\n","    for (img,mask) in train_generator:\n","        img,mask = adjustData(img,mask,flag_multi_class,num_class)\n","        yield (img,mask)\n","\n","def testGenerator(test_path,target_size = (224,224),flag_multi_class = False,as_gray = True): \n","    for filename in os.listdir(test_path):\n","        img = io.imread(os.path.join(test_path,filename),as_gray = as_gray) \n","        img = img / 255.\n","        img = trans.resize(img,target_size)\n","        img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n","        img = np.reshape(img,(1,)+img.shape)\n","        yield img\n","\n","\n","def labelVisualize(num_class,color_dict,img):\n","    img = img[:,:,0] if len(img.shape) == 3 else img\n","    img_out = np.zeros(img.shape + (3,))\n","    for i in range(num_class):\n","        img_out[img == i,:] = color_dict[i]\n","    return img_out / 255\n","\n","\n","def saveResult(save_path,npyfile,test_path, flag_multi_class = False,num_class = 2):\n","    file_names = os.listdir(test_path)\n","    for i,item in enumerate(npyfile):\n","        img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n","        io.imsave(os.path.join(save_path,file_names[i]),img)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"NSH-V_uljkQ7","colab_type":"text"},"cell_type":"markdown","source":["Its time to train and test the Dropout_UNET model.\n","\n","Data augmentation:\n","\n","In deep learning tasks, a lot of data is need to train DNN model, when the dataset is not big enough, data augmentation should be applied. Keras.preprocessing.image.ImageDataGenerator is a data generator, which can feed the DNN with data like : (data,label), it can also do data augmentation at the same time. It is very convenient for us to use keras.preprocessing.image.ImageDataGenerator to do data augmentation by implement image rotation, shift, rescale and so on. See keras documentation for detail. For image segmentation tasks, the image and mask must be transformed together!! You can define your data generator and visualize your data augmentation result by setyting save_to_dir = \"your path\". "]},{"metadata":{"id":"DSCX6yYcjn67","colab_type":"code","colab":{}},"cell_type":"code","source":["test_path = \"data/membrane/test\"\n","save_path = \"data/membrane/result\"\n","\n","data_gen_args = dict(rotation_range=0.2,\n","                    width_shift_range=0.05,\n","                    height_shift_range=0.05,\n","                    shear_range=0.05,\n","                    zoom_range=0.05,\n","                    horizontal_flip=True,\n","                    fill_mode='nearest')\n","\n","myGene = trainGenerator(2,'data/membrane/train','image','label',data_gen_args,save_to_dir = None) #batch size = 2 here\n","\n","\n","model = unet()\n","model_checkpoint = ModelCheckpoint('dropout_unet.hdf5', monitor='loss',verbose=1, save_best_only=True)\n","model.fit_generator(myGene,steps_per_epoch=2000,epochs=200, verbose=1, callbacks=[model_checkpoint]) \n","testGene = testGenerator(test_path)\n","model = unet()\n","model.load_weights(\"dropout_unet.hdf5\")\n","results = model.predict_generator(testGene,7714,verbose=1, workers=1, use_multiprocessing=False) \n","#steps per epoch is the no. of samples in test image, here it is 7714.\n","saveResult(save_path, results, test_path)"],"execution_count":0,"outputs":[]}]}